{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossflow 101\n",
    "An introduction to the fundamentals of Crossflow\n",
    "\n",
    "Workflows are a common feature of much computational science. In a workflow, the work to be done requires more than one piece of software, and the output from one becomes the input to the next, in some form of chain. Classically one would write some sort of bash script or similar to do the job, e.g.:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "input_file=input.dat\n",
    "intermediate_file=intermediate.dat\n",
    "result_file=result.dat\n",
    "\n",
    "executable1 -i $input_file -o $intermediate_file\n",
    "executable2 -i $intermediate_file -o $result_file\n",
    "\n",
    "```\n",
    "This is OK for basic use but:\n",
    "* what if your workflow has loops, conditional executions, etc?\n",
    "* what happens if you want to do things at scale?\n",
    "\n",
    "Crossflow is designed to make this easier. Key points are:\n",
    "\n",
    "1. The workflow becomes a Python program, and can make use of all programming workflow constructs (loops, if/then/else, etc.)\n",
    "2. To do this, it provides a simple approach to turning command line tools into Python functions - this is `crossflow.kernels`.\n",
    "3. It provides a way to pass data between these functions without relying on the filesystem - this is `crossflow.filehandling`.\n",
    "4. It provides a way to hand the processing of individual workflow steps out to a distributed cluster of workers - this is `crossflow.clients`.\n",
    "\n",
    "Here we look at each of these components in turn.\n",
    "\n",
    "----------------\n",
    "\n",
    "## Crossflow FileHandles\n",
    "\n",
    "Command line tools typically take file *names* as arguments:\n",
    "```\n",
    "executable -i input.dat -o output.dat\n",
    "```\n",
    "This has issues if you want to do the computing on a distributed system where there is no shared filesystem. Crossflow `FileHandles` wrap data files as serialisable Python objects that can be safely passed around a network. Crossflow `Kernels` (see below) natively understand these as the equivalents of the filenames one would use for the equivalent command line call.\n",
    "\n",
    "### Creating crossflow.FileHandles\n",
    "\n",
    "To convert a file to a `crossflow.FileHandle` you first create an instance of a `crossflow.FileHandler` and then use the `FileHandler`'s .load() method:\n",
    "\n",
    "```python\n",
    "from crossflow import filehandling\n",
    "fh = filehandling.FileHandler()\n",
    "input = fh.load('input.txt')\n",
    "```\n",
    "\n",
    "Crossflow supports a variety of `FileHandler`s. The default stores the contents of the file in memory. If you are passing a lot of data around, or the files are very large, there are alternatives.\n",
    "\n",
    "If you have a directory somewhere which you know will be visible to all your workers (e.g. a shared NFS directory), you can use that as the staging post:\n",
    "\n",
    "```python\n",
    "fh_shared = filehandling.FileHandler('/tmp')\n",
    "input = fh_shared.load('input.txt')\n",
    "```\n",
    "\n",
    "Alternatively if you have access to Amazon S3, you can use an s3 bucket for the same purpose:\n",
    "```python\n",
    "fh_s3 = filehandling.FileHandler('s3://my_account.crossflow_bucket/')\n",
    "input = fh_s3.load('input.txt')\n",
    "```\n",
    "\n",
    "Under the hood, `crossflow.filehandling` uses the Python [fsspec](https://www.anaconda.com/fsspec-remote-caching/) package, and some of the additional storage backends that supports may work for you as well.\n",
    "\n",
    "### Methods of crossflow.FileHandles\n",
    "\n",
    "The .save() method of a `crossflow.FileHandle` creates a conventional local file with the object's contents:\n",
    "\n",
    "```python\n",
    "input.save('input_copy.txt')\n",
    "```\n",
    "\n",
    "The .as_file() method returns the name of a (maybe temporary) file with the object's contents, that can be passed to functions that expect a conventional file name:\n",
    "\n",
    "```python\n",
    "with open(input.as_file()) as f:\n",
    "    data = f.read()\n",
    "```\n",
    "\n",
    "--------------------\n",
    "## Crossflow Kernels\n",
    "\n",
    "The `crossflow.kernels` subpackage provides methods to turn tools that would usually be used via the command line into Python functions. The basic concept is that a tool that is used from the commmand line something like:\n",
    "```bash\n",
    "my_tool -i input.dat -o output.dat\n",
    "```\n",
    "becomes, in Python:\n",
    "```\n",
    "output = my_tool_kernel(input)\n",
    "```\n",
    "`\n",
    "Where my_tool_kernel` is a `crossflow.SubprocessKernel` for `my_tool`, `input` is a `crossflow.FileHandle` for `input.dat` and `output` is a `crossflow.FileHandle` for `output.dat`.\n",
    "\n",
    "### Creating a crossflow.SubprocessKernel\n",
    "\n",
    "This is a three step process:\n",
    "\n",
    "1. The kernel is created on the basis of a `template`, a string with a generalised version of the command you wish to execute.\n",
    "2. The inputs for the kernel are specified.\n",
    "3. The outputs from the kernel are specified.\n",
    "\n",
    "Thus:\n",
    "```python\n",
    "my_tool_kernel = crossflow.kernels.SubprocessKernel('my_tool -i x.in -o x.out')\n",
    "my_tool_kernel.set_inputs(['x.in'])\n",
    "my_tool_kernel.set_outputs(['x.out'])\n",
    "```\n",
    "Note that the names of files used in the template string are arbitrary, 'my_tool -i a -o b' would do just as well, as long as the corresponding names ('a', 'b') were used in .set_inputs() and .set_outputs().\n",
    "\n",
    "If the tool takes multiple files as inputs, and/or produces multiple output files, the process is the same:\n",
    "```python\n",
    "my_othertool_kernel = crossflow.kernels.SubprocessKernel('my_othertool -x x.in -y y.in -o x.out -l logfile')\n",
    "my_othertool_kernel.set_inputs(['x.in', 'y.in'])\n",
    "my_othertool_kernel.set_outputs(['x.out', 'logfile'])\n",
    "```\n",
    "\n",
    "There is no restriction on the order that inputs and outputs are specified in the template string, but the resulting kernel will expect its inputs to be provided in the order they are given in .set_inputs() and the tuple of outputs the kernel produces will be in the order they are specified in .set_outputs().\n",
    "\n",
    "For more advanced aspects of `SubprocessKernel` creation, see elsewhere.\n",
    "\n",
    "### Running a crossflow.SubprocessKernel\n",
    "\n",
    "Although it is primarily expected that kernels will be run via a `crossflow.Client`, they can also be executed directly, via their .run() method:\n",
    "```python\n",
    "output, logfile = my_othertool.run(x, y) # assuming x and y are existing FileHandle objects\n",
    "```\n",
    "\n",
    "--------------------\n",
    "## Crossflow Clients\n",
    "The `crossflow.clients` sub-package provides a Client through which one can execute kernels on distributed resources. At its heart a `crossflow.clients.Client()` is a [dask.distributed](https://distributed.dask.org/en/latest/) client, and new users are strongly encouraged to read the documentation there to understand how Crossflow works.\n",
    "\n",
    "### Creating a crossflow.Client\n",
    "\n",
    "A Crossflow client provides access to a cluster of workers. These may be remote machines, or a set of worker processes on the current compute resource (see the dask documentation for more details). The cluster may be already up and running, in which case the crossflow.Client just needs to know where it is (the address of its scheduler):\n",
    "\n",
    "```python\n",
    "my_client = crossflow.clients.Client(scheduler_file='scheduler.json')\n",
    "```\n",
    "\n",
    "Alternatively (typically for testing purposes), a local cluster may be created on the fly, to serve the Client:\n",
    "```python\n",
    "my_client = crossflow.clients.Client(local=True)\n",
    "```\n",
    "\n",
    "### Using a crossflow.Client\n",
    "\n",
    "A crossflow.Kernel is sent to a crossflow.Client for execution using the client's .submit() or .map() method.\n",
    "\n",
    "\n",
    "Running a single job:\n",
    "```python\n",
    "output_future, logfile_future = my_client.submit(my_othertool_kernel, x, y)\n",
    "```\n",
    "Compare with the interactive version above:\n",
    "1. The kernel argument omits the .run() part.\n",
    "2. The outputs (output_future, logfile_future) are now Futures - again, see the dask documentation for more detail, but also notice the difference: dask's .submit() method always returns a single Future, while crossflow's one returns one Future per expected output.\n",
    "\n",
    "Running a set of jobs in parallel:\n",
    "```python\n",
    "xs = [x1, x2, x3, x4]\n",
    "ys = [y1, y2, y3, y4]\n",
    "output_futures, logfile_futures = my_client.map(my_othertool_kernel, xs, ys)\n",
    "```\n",
    "In this case the .map() method returns lists of Futures. The individual jobs are scheduled to the workers in the compute cluster in whatever way is most efficient, if there are enough of them to run all four jobs in parallel, they will.\n",
    "\n",
    "-------------\n",
    "## A simple demonstration\n",
    "\n",
    "Here we create a `SubprocessKernel` to reverse the order of the lines in a file, create a `Filehandle` for an input file, submit the job to a local `Client`, and then retrieve and view the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 9\n",
      "line 8\n",
      "line 7\n",
      "line 6\n",
      "line 5\n",
      "line 4\n",
      "line 3\n",
      "line 2\n",
      "line 1\n",
      "line 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from crossflow import clients, kernels, filehandling\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a short text file:\n",
    "here = Path('.')\n",
    "inp_file = here /'input.txt'\n",
    "with inp_file.open('w') as f:\n",
    "    for i in range(10):\n",
    "        f.write('line {}\\n'.format(i))\n",
    "\n",
    "# Create a SubprocessKernel that will reverse the lines in a file:\n",
    "reverser = kernels.SubprocessKernel('tail -r input > output')\n",
    "reverser.set_inputs(['input'])\n",
    "reverser.set_outputs(['output'])\n",
    "\n",
    "# Convert the input datafile into a Crossflow FileHandle object:\n",
    "fh = filehandling.FileHandler()\n",
    "inp = fh.load(inp_file)\n",
    "\n",
    "# Create a local client to run the job, and submit it:\n",
    "client = clients.Client(local=True)\n",
    "output = client.submit(reverser, inp)\n",
    "\n",
    "# output is a Future; collect its result(), convert this FileHandle object to a file, and list its contents:\n",
    "output_file = here / 'joined.txt'\n",
    "output.result().save(output_file)\n",
    "print(output_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
