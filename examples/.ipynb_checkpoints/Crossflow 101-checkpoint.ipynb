{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossflow 101\n",
    "An introduction to the fundamentals of Crossflow\n",
    "\n",
    "Workflows are a common feature of much computational science. In a workflow, the work to be done requires more than one piece of software, and the output from one becomes the input to the next, in some form of chain. Classically one would write some sort of bash script or similar to do the job, e.g.:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "input_file=input.dat\n",
    "intermediate_file=intermediate.dat\n",
    "result_file=result.dat\n",
    "\n",
    "executable1 -i $input_file -o $intermediate_file\n",
    "executable2 -i $intermediate_file -o $result_file\n",
    "\n",
    "```\n",
    "This is OK for basic use but:\n",
    "* what if your workflow has loops, conditional executions, etc?\n",
    "* what happens if you want to do things at scale?\n",
    "\n",
    "Crossflow is designed to make this easier. Key points are:\n",
    "\n",
    "1. The workflow becomes a Python program, and can make use of all programming workflow constructs (loops, if/then/else, etc.)\n",
    "2. To do this, it provides a simple approach to turning command line tools into Python functions - this is `crossflow.kernels`.\n",
    "3. It provides a way to hand the processing of individual workflow steps out to a distributed cluster of workers - this is `crossflow.clients`.\n",
    "\n",
    "Here we look at each of these components in turn.\n",
    "\n",
    "--------------------\n",
    "## Crossflow Kernels\n",
    "\n",
    "The `crossflow.kernels` subpackage provides methods to turn tools that would usually be used via the command line into Python functions. The basic concept is that a tool that is used from the commmand line something like:\n",
    "```bash\n",
    "my_tool -i input.dat -o output.dat\n",
    "```\n",
    "becomes, in Python:\n",
    "```\n",
    "output = my_tool_kernel.run('input.dat')\n",
    "```\n",
    "`\n",
    "Where my_tool_kernel` is a `crossflow.SubprocessKernel` for `my_tool` and `output` is a `crossflow.FileHandle`, which behaves much like a Python `Path` object (see [here](https://docs.python.org/3/library/pathlib.html)).\n",
    "\n",
    "### Creating a crossflow.SubprocessKernel\n",
    "\n",
    "This is a three step process:\n",
    "\n",
    "1. The kernel is created on the basis of a `template`, a string with a generalised version of the command you wish to execute.\n",
    "2. The inputs for the kernel are specified.\n",
    "3. The outputs from the kernel are specified.\n",
    "\n",
    "Thus:\n",
    "```python\n",
    "my_tool_kernel = crossflow.kernels.SubprocessKernel('my_tool -i x.in -o x.out')\n",
    "my_tool_kernel.set_inputs(['x.in'])\n",
    "my_tool_kernel.set_outputs(['x.out'])\n",
    "```\n",
    "Note that the names of files used in the template string are arbitrary, 'my_tool -i a -o b' would do just as well, as long as the corresponding names ('a', 'b') were used in .set_inputs() and .set_outputs().\n",
    "\n",
    "If the tool takes multiple files as inputs, and/or produces multiple output files, the process is the same:\n",
    "```python\n",
    "my_othertool_kernel = crossflow.kernels.SubprocessKernel('my_othertool -x x.in -y y.in -o x.out -l logfile')\n",
    "my_othertool_kernel.set_inputs(['x.in', 'y.in'])\n",
    "my_othertool_kernel.set_outputs(['x.out', 'logfile'])\n",
    "```\n",
    "\n",
    "There is no restriction on the order that inputs and outputs are specified in the template string, but the resulting kernel will expect its inputs to be provided in the order they are given in .set_inputs() and the tuple of outputs the kernel produces will be in the order they are specified in .set_outputs().\n",
    "\n",
    "For more advanced aspects of `SubprocessKernel` creation, see elsewhere.\n",
    "\n",
    "### Running a crossflow.SubprocessKernel\n",
    "\n",
    "Although it is primarily expected that kernels will be run via a `crossflow.Client`, they can also be executed directly, via their .run() method:\n",
    "```python\n",
    "output, logfile = my_othertool.run(x, y)\n",
    "```\n",
    "As explained above, `output` and `logfile` will be 'Path-like' objects (but with more limited functionality than real `Path` objects). So to save the output to a local file:\n",
    "```python\n",
    "output.save('output.dat')\n",
    "```\n",
    "Or to look at the contents of the logfile directly:\n",
    "```python\n",
    "print(logfile.read_text())\n",
    "```\n",
    "\n",
    "--------------------\n",
    "## Crossflow Clients\n",
    "The `crossflow.clients` sub-package provides a Client through which one can execute kernels on distributed resources. At its heart a `crossflow.clients.Client()` is a [dask.distributed](https://distributed.dask.org/en/latest/) client, and new users are strongly encouraged to read the documentation there to understand how Crossflow works.\n",
    "\n",
    "### Creating a crossflow.Client\n",
    "\n",
    "A Crossflow client provides access to a cluster of workers. These may be remote machines, or a set of worker processes on the current compute resource (see the dask documentation for more details). The cluster may be already up and running, in which case the crossflow.Client just needs to know where it is (the address of its scheduler):\n",
    "\n",
    "```python\n",
    "my_client = crossflow.clients.Client(scheduler_file='scheduler.json')\n",
    "```\n",
    "\n",
    "Alternatively (typically for testing purposes), a local cluster may be created on the fly, to serve the Client:\n",
    "```python\n",
    "my_client = crossflow.clients.Client(local=True)\n",
    "```\n",
    "\n",
    "### Using a crossflow.Client\n",
    "\n",
    "A crossflow.Kernel is sent to a crossflow.Client for execution using the client's .submit() or .map() method.\n",
    "\n",
    "\n",
    "#### Running a single job:\n",
    "```python\n",
    "output_future, logfile_future = my_client.submit(my_othertool_kernel, x, y)\n",
    "```\n",
    "Compare with the interactive version above:\n",
    "1. The kernel argument omits the .run() part.\n",
    "2. The outputs (output_future, logfile_future) are now Futures - again, see the dask documentation for more detail, but also notice the difference: dask's .submit() method always returns a single Future, while crossflow's one returns one Future per expected output.\n",
    "\n",
    "#### Running a set of jobs in parallel:\n",
    "```python\n",
    "xs = [x1, x2, x3, x4]\n",
    "ys = [y1, y2, y3, y4]\n",
    "output_futures, logfile_futures = my_client.map(my_othertool_kernel, xs, ys)\n",
    "```\n",
    "In this case the .map() method returns lists of Futures. The individual jobs are scheduled to the workers in the compute cluster in whatever way is most efficient, if there are enough of them to run all four jobs in parallel, they will.\n",
    "\n",
    "-------------\n",
    "## A simple demonstration\n",
    "\n",
    "Here we create a `SubprocessKernel` to reverse the order of the lines in a file, submit the job to a local `Client`, and then retrieve and view the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossflow import clients, kernels\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a short text file:\n",
    "here = Path('.')\n",
    "inp_file = here /'input.txt'\n",
    "with inp_file.open('w') as f:\n",
    "    for i in range(10):\n",
    "        f.write('line {}\\n'.format(i))\n",
    "\n",
    "# Create a SubprocessKernel that will reverse the lines in a file:\n",
    "reverser = kernels.SubprocessKernel('rev input > output')\n",
    "reverser.set_inputs(['input'])\n",
    "reverser.set_outputs(['output'])\n",
    "\n",
    "\n",
    "# Create a local client to run the job, and submit it:\n",
    "client = clients.Client(local=True)\n",
    "output = client.submit(reverser, inp_file)\n",
    "\n",
    "# output is a Future; collect its result(), which is a 'Path-like' FileHandle, and list its contents:\n",
    "output_file = here / 'joined.txt'\n",
    "print(output.result().read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
